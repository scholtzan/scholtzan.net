+++
title = "Speeding up Mozilla's automatic experiment analysis"
date = "2020-12-04"
type = "post"
tags = ["Mozilla", "Open Source", "Argo", "Python", "Jetstream", "Dask"]
+++

_This post describes my recent work on [jetstream](https://github.com/mozilla/jetstream) as part of my day job at Mozilla. In particular, I'll describe how [Argo](https://argoproj.github.io/) and [Dask](https://dask.org/) were used to scale up a system that processes data dozens of different [experiments](https://firefox-source-docs.mozilla.org/browser/urlbar/experiments.html#experiments) in Firefox daily and on-demand. This work has been co-developed with my colleague [Tim Smith](https://github.com/tdsmith)._

Beginning of 2020 the development of a new experiment analysis infrastructure was launched at Mozilla which should help scaling up the number of experiments done in Firefox and reduce the involvement of data scientists necessary for each experiment.
The entire infrastructure consists of different components and services, with [jetstream](https://github.com/mozilla/jetstream) being the component that automatically analyses collected telemetry data of clients enrolled in experiments. As the number of experiments started to increase, some of which require processing an extensive amount of data, running the daily automatic analyses started to take very long. In some cases completing the experiment analyses for a single day took over 23 hours. To provide analysis results without significant delay and speed up the entire analysis process, it was time to make some architectural changes and parallelize jetstream's experiment analysis. [Argo](https://argoproj.github.io/) looked perfect for parallelizing the analysis on a higher level and in combination with [Dask](https://dask.org/) for parallelizing lower-level calculations, we were able to significantly reduce the analysis runtime.


## Background: jetstream

![Jetstream Overview](/img/jetstream-overview.png)
*Jetstream Overview.*

At Mozilla, experiments are managed by the [Experimenter](https://github.com/mozilla/experimenter) service and delivered to Firefox clients via [Normandy](https://mozilla.github.io/normandy). Product stakeholders and data scientists are interested in how specific metrics, such as the number of hours Firefox has been used or number of searches done, are affected by these experiments. Jetstream is [scheduled in Airflow](https://github.com/mozilla/telemetry-airflow/blob/master/dags/jetstream.py) to calculate metrics and apply statistical treatments to collected experiment [telemetry data](https://docs.telemetry.mozilla.org/tools/guiding_principles.html) for different analysis windows. Our telemetry data as well as all of the generated data artefacts are stored in [BigQuery](https://cloud.google.com/bigquery). 

The generated data artefacts are visualized in dashboards that allow stakeholders to see results and changes. Data scientists have direct access to the datasets generated by jetstream to allow for custom analysis. For calculating metrics and [statistics](https://github.com/mozilla/jetstream/wiki/Statistics) jetstream uses the [mozanalysis library](https://github.com/mozilla/mozanalysis) Python library.
While there are a few [pre-defined metrics and statistics](https://github.com/mozilla/jetstream/tree/main/jetstream/config) that are calculated for every experiment, it is also possible to provide custom configurations with additional metrics and statistics. These configurations are stored in the [jetstream-config repository](https://github.com/mozilla/jetstream-config) and automatically detected and applied by jetstream. A more detailed architectural overview of jetstream and how it fits in the experiment analysis infrastructure is available in the [repository Wiki](https://github.com/mozilla/jetstream/wiki/Architecture).

When analyzing experiments, the following steps are executed for each experiment:

![Jetstream Analysis Steps](https://raw.githubusercontent.com/mozilla/jetstream/main/docs/analysis-steps.png)
*Jetstream experiment analysis steps.*

A default configuration and, if defined, a custom configuration provided via the [jetstream-config repository](https://github.com/mozilla/jetstream-config) are parsed and used for analysis. The experiment definition and config parameters are used to run some checks to determine if the experiment can be analyzed. These checks include, for example, validating start dates, end dates and enrollment periods.

If the experiment is valid, then metrics are calculated for each analysis period (daily, weekly, overall) and written to BigQuery. Metrics are either specified or a reference to existing metrics defined in mozanalysis is provided in the configuration files. Next, for each segment, first pre-treatments are applied to the metrics data which is then used to calculate statistics. Statistics data is written to BigQuery and later exported to GCS as JSON.

Initially, jetstream was set up to run on Airflow, constraining it to a single slot with limited amount of memory available which was limiting the analysis speed. For each of these experiments, up to 12 GB of memory are required to calculate statistics. As the number of experiments analyses per day increased simply running the analyses of these experiments in a single Kubernetes pod was not performant anymore. As Airflow does not support creating tasks dynamically during runtime, it was not possible to create separate tasks for each experiment analysis. A different approach was needed.


## Parallelizing experiment analyses using Argo

After some research and trying out different approaches, [Argo](https://argoproj.github.io/) turned out to be a great for distributing analyses for different experiments.  
Argo is a light-weight workflow engine for orchestrating parallel jobs on Kubernetes and is capable of creating tasks dynamically that will be executed in parallel. Using Argo, the analyses for different experiments and analysis dates can be split into separate jobs that run in parallel.

The setup was quite straightforward: a Kubernetes cluster needed to be set up and Argo was installed on the cluster by following the [installation guide](https://github.com/argoproj/argo/blob/master/docs/quick-start.md). When creating the cluster, BigQuery and Compute Engine read/write permissions needed to be enabled to ensure pods have sufficient access to our telemetry data and can submit Argo workflows.

[Workflows](https://argoproj.github.io/argo/workflow-concepts/) are one of the core Argo concepts. Workflows define what is being executed. They are written as `yaml` and generally consist of an entrypoint and a list of templates defining the work to be done in each step.

The workflow spec that is used for the experiment analysis in jetstream is shown in the following:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: jetstream-
spec:
  entrypoint: jetstream
  arguments:
    parameters:
    - name: experiments  # set dynamically by jetstream when workflow gets deployed
    - name: project_id
    - name: dataset_id
  templates:
  - name: jetstream
    parallelism: 5  # run up to 5 containers in parallel at the same time
    inputs:
      parameters:
        - name: experiments
    steps:
    - - name: analyse-experiment
        template: analyse-experiment  
        arguments:
          parameters:
          - name: slug
            value: "{{item.slug}}"
          - name: date
            value: "{{item.date}}"
        withParam: "{{inputs.parameters.experiments}}"  # process these experiments in parallel
        continueOn:
          failed: true
    - - name: export-statistics
        template: export-statistics
        arguments:
          parameters:
          - name: slug
            value: "{{item.slug}}"
        withParam: "{{inputs.parameters.experiments}}" 

  - name: analyse-experiment
    inputs:
      parameters:
      - name: date
      - name: slug
    container:
      image: gcr.io/moz-fx-data-experiments/jetstream:latest
      command: [
        bin/entrypoint, run, "--date={{inputs.parameters.date}}", 
        "--experiment_slug={{inputs.parameters.slug}}", 
        "--dataset_id={{workflow.parameters.dataset_id}}", 
        "--project_id={{workflow.parameters.project_id}}"
      ]
      resources:
        requests:
          memory: 10Gi   # make sure there is at least 10Gb of memory available for the task
        limits:
          cpu: 4  # limit to 4 cores
    retryStrategy:
      limit: 3  # execute a container max. 3x; sometimes a container run might fail due to limited resources
      retryPolicy: "Always"
      backoff:
        duration: "1m"
        factor: 2
        maxDuration: "5m"

  - name: export-statistics
    inputs:
      parameters:
        - name: slug
    container:
      image: gcr.io/moz-fx-data-experiments/jetstream:latest
      command: [
        bin/entrypoint, export-statistics-to-json, 
        "--dataset_id={{workflow.parameters.dataset_id}}", 
        "--project_id={{workflow.parameters.project_id}}",
        "--experiment_slug={{inputs.parameters.slug}}"
      ]
    activeDeadlineSeconds: 600   # terminate container template after 10 minutes
```

When jetstream runs the daily analyses, it fetches all active experiments from Experimenter and injects these experiments as `parameter` into the workflow spec. Each experiment has a unique slug and a date for which data should be processed. Additionally, the GCP project name and destination dataset in BigQuery where data should be written to need to be specified as `parameter`s. 

The workflow spec defines two steps that are executed for each experiment and date: `analyse-experiment` and `export-statistics`. In the `analyse-experiment` metrics and statistics are calculated and written to BigQuery. `export-statistics` exports all statistics data that has been written to BigQuery as JSON to GCS to make it available to our dashboard tools. Running these steps for each experiment will be done in parallel. Jetstream provides an entrypoint script that allows to specify what the Jetstream container should execute. The `run` and `export-statistics-to-json` are used by the steps defined in the spec.

Up to 5 experiment analyses will get executed in parallel. This is to ensure available cluster resources aren't exceeded when too many experiments need to be analysed at the same time. The spec also allows to manage Kubernetes resources. For the `analyse-experiment` step, which is potentially quite compute and memory-usage intensive, the spec ensures that the container has at least 10 Gb of memory available and up to 4 CPU cores. Also, if the step happens to fail, the specified `retryStrategy` ensures that it the container will get executed up to 3 times.

As these workflow parameters might change between runs, jetstream injects them into the workflow spec right before it submits the workflow to Argo. For submitting workflows to Argo, we use the [`argo-client-python`](https://github.com/argoproj-labs/argo-client-python) library. 

In addition to daily analysis runs, jetstream also needed to support running analyses on-demand. When a config in jetstream-config changes or a new one is added, all analyses of the affected experiment since the time it had been launched need to be re-executed. The defined workflow also supports this use case. Instead of having a list of different experiments with the same analysis as `parameter`, it will inject a list of the 

[ toot]tuple slug date




* solution
	* setup
	* Argo
		* some general information
		* workflows
		* python client
		* benchmmarks
		* dashboard
	* Dask
		* some general information
		* adding Dask
			* rewriting without nesting
			* pickle problems
		* benchmakrs
* error handling?
	* bigquery logging
* pain points
	* permissions
* future work
	* dask kubernetes